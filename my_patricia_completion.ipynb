{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2c40e7c-2430-4386-aa52-1aaf3313af02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /Users/sanlin/tensorflow-prj/.env/lib/python3.8/site-packages (3.1.0)\n",
      "Requirement already satisfied: filelock in /Users/sanlin/tensorflow-prj/.env/lib/python3.8/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/sanlin/tensorflow-prj/.env/lib/python3.8/site-packages (from datasets) (1.23.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/sanlin/tensorflow-prj/.env/lib/python3.8/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/sanlin/tensorflow-prj/.env/lib/python3.8/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/sanlin/tensorflow-prj/.env/lib/python3.8/site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/sanlin/tensorflow-prj/.env/lib/python3.8/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/sanlin/tensorflow-prj/.env/lib/python3.8/site-packages (from datasets) (4.67.0)\n",
      "Requirement already satisfied: xxhash in /Users/sanlin/tensorflow-prj/.env/lib/python3.8/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/sanlin/tensorflow-prj/.env/lib/python3.8/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /Users/sanlin/tensorflow-prj/.env/lib/python3.8/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /Users/sanlin/tensorflow-prj/.env/lib/python3.8/site-packages (from datasets) (3.10.11)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /Users/sanlin/tensorflow-prj/.env/lib/python3.8/site-packages (from datasets) (0.26.2)\n",
      "Requirement already satisfied: packaging in /Users/sanlin/tensorflow-prj/.env/lib/python3.8/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/sanlin/tensorflow-prj/.env/lib/python3.8/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/sanlin/tensorflow-prj/.env/lib/python3.8/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/sanlin/tensorflow-prj/.env/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/sanlin/tensorflow-prj/.env/lib/python3.8/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/sanlin/tensorflow-prj/.env/lib/python3.8/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/sanlin/tensorflow-prj/.env/lib/python3.8/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /Users/sanlin/tensorflow-prj/.env/lib/python3.8/site-packages (from aiohttp->datasets) (1.15.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/sanlin/tensorflow-prj/.env/lib/python3.8/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/sanlin/tensorflow-prj/.env/lib/python3.8/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sanlin/tensorflow-prj/.env/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sanlin/tensorflow-prj/.env/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sanlin/tensorflow-prj/.env/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sanlin/tensorflow-prj/.env/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/sanlin/tensorflow-prj/.env/lib/python3.8/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/sanlin/tensorflow-prj/.env/lib/python3.8/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/sanlin/tensorflow-prj/.env/lib/python3.8/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/sanlin/tensorflow-prj/.env/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/sanlin/tensorflow-prj/.env/lib/python3.8/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: myanmartools in /Users/sanlin/tensorflow-prj/.env/lib/python3.8/site-packages (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install myanmartools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8383039-38c0-4106-ae82-cba180313e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'url', 'title', 'text'],\n",
      "    num_rows: 110833\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"wikipedia\", language=\"my\",\n",
    "                  date=\"20250120\", trust_remote_code=True, split=\"train\")\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b742d27d-d36f-4917-b5fa-0a60a73a2792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "cons = \"\\u1000-\\u1021\"\n",
    "independent_vowel = \"[\\u1023-\\u1027\\u1029\\u102a]\"\n",
    "cons_init = f\"[{cons}\\u1025\\u1026\\u1027\\u1029\\u103f]\"\n",
    "# consonants + independent_vowel (!caution! 103F is need extra requirement)\n",
    "medial = \"[\\u103b-\\u103e]{,4}\"\n",
    "vowel = \"[\\u102b-\\u1032]{1,2}\\u103a?\"\n",
    "asat = \"\\u103a\"\n",
    "virama = \"\\u1039\"\n",
    "cons_asat = f\"[{cons}](?:{asat}{virama}|[{asat}{virama}])\"\n",
    "kinzi = f\"[\\u1004\\u101b]{asat}{virama}\"\n",
    "tone = \"[\\u1036\\u1037\\u1038]{1,2}\"\n",
    "legaund = f\"\\u104e\\u1004{asat}\\u1038|\\u104e\\u1004{asat}\"\n",
    "symbol1 = f\"[\\u104c\\u104d\\u104f]|{legaund}|\\u104e\"\n",
    "symbol2 = f\"\\u103b{asat}\"\n",
    "contra1 = \"\\u103b\\u102c\"\n",
    "digit = \"[\\u1040-\\u1049,\\.]+\"\n",
    "# others = \"(?![a-zA-Z0-9])[\\t-~\\x0a\\x0d\\u104a\\u104b]\"\n",
    "others = \"[ \\u104a\\u104b]\"\n",
    "# cons + medial* + vowel* + cons_asat? + tone*\n",
    "# syllable_based_pattern = f\"(?<={cons_init}){medial}(?:{vowel})?\" + \\\n",
    "#     f\"(?:{cons_asat}(?:{contra1})?)?(?:{tone})?\"\n",
    "syllable_based_pattern = f\"{cons_init}{medial}(?:{vowel})?\" + \\\n",
    "     f\"(?:{cons_asat}(?:{contra1})?)?(?:{tone})?\"\n",
    "\n",
    "\n",
    "def extract_syllables(text):\n",
    "    # Updated regular expression to handle complex Burmese syllable structures\n",
    "    #text = text.replace(f'{asat}{virama}', asat)\n",
    "    #text = text.replace(f'{virama}', asat)\n",
    "    text = text.replace('\\u1037\\u103a','\\u103a\\u1037')\n",
    "    syllable_pattern = (\n",
    "        rf\"{symbol1}|\"\n",
    "        rf\"{symbol2}|\"\n",
    "        rf\"{syllable_based_pattern}|\"\n",
    "        rf\"{independent_vowel}|\"\n",
    "        rf\"{others}\"\n",
    "        # rf\"{cons_init}\"\n",
    "    )\n",
    "    syllables = re.findall(syllable_pattern, text)\n",
    "    #syllables = [syll for syll in syllables if syll != '']\n",
    "    return list(filter(None, syllables))\n",
    "\n",
    "syllable_break_pattern = f\"(?<={cons_init}){medial}(?:{vowel})?\" + \\\n",
    "    f\"(?:{cons_asat}(?:{contra1})?)?(?:{tone})?\"\n",
    "\n",
    "def extract_syllable_to_2parts(text):\n",
    "    # Updated regular expression to handle complex Burmese syllable structures\n",
    "    #text = text.replace(f'{asat}{virama}', asat)\n",
    "    #text = text.replace(f'{virama}', asat)\n",
    "    syllable_pattern = (\n",
    "        rf\"{symbol1}|\"\n",
    "        rf\"{symbol2}|\"\n",
    "        rf\"{syllable_break_pattern}|\"\n",
    "        rf\"{independent_vowel}|\"\n",
    "        rf\"{others}|\"\n",
    "        rf\"{cons_init}\"\n",
    "    )\n",
    "    syllables = re.findall(syllable_pattern, text)\n",
    "    syllables = list(filter(None, syllables))\n",
    "    return syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1385e78f-f261-4af2-820a-61236c2cad50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "text1 = \"သင်္ကြန်၊ မန္တလေး၊ ယောက်ျား၊ လက်ချ်မီး၊ သင်္ချာ။ ၎င်း၊ ၎င်၊ ၎၊ ၁၀.၁ ၁၀,၃၀၀.၀၀abc124de\\t~ဦးဦ\"\n",
    "text2 = \"မြန်မာစာကို မြန်မာတစ်ယောက်က စစ်သည်။\"\n",
    "extract_syllables(text1)\n",
    "extract_syllable_to_2parts(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c52e9779-bf2c-4b3c-8b9b-ee4f60683467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9997572675217831\n"
     ]
    }
   ],
   "source": [
    "from myanmartools import ZawgyiDetector\n",
    "\n",
    "zg_detector = ZawgyiDetector()\n",
    "score = zg_detector.get_zawgyi_probability('မ္း')\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b19f584-ed35-442c-adf3-9713fe4846f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "syllables = []\n",
    "from tqdm.notebook import tqdm\n",
    "syllables_map = {}\n",
    "i = 0\n",
    "syllables_count = 0\n",
    "syllables_vocab = []\n",
    "syllables_parts_vocab = []\n",
    "sentences = []\n",
    "syllables_hash = {}\n",
    "ds_size = ds.num_rows\n",
    "#ds_size = 1\n",
    "for data in tqdm(ds, total=ds_size, desc=\"Processing records\"):\n",
    "    i = i + 1\n",
    "    score = zg_detector.get_zawgyi_probability(data[\"text\"])\n",
    "    if score > 0.80:\n",
    "        print(f\"zawgyi encoded sentence skipped with scoreb {score}\")\n",
    "        continue\n",
    "    syllables_out = extract_syllable_to_2parts(data[\"text\"])\n",
    "    for syll in syllables_out:\n",
    "        if syll in syllables_hash:\n",
    "            syllables_hash[syll][\"count\"] += 1\n",
    "        else:\n",
    "            syllables_hash[syll] = {\"part\":syll, \"count\":1}\n",
    "    syllables = syllables + syllables_out\n",
    "    syllables_count= syllables_count + len(syllables_out)\n",
    "    if i%1000 == 0:\n",
    "        syllables = sorted(set(syllables))\n",
    "    if i > ds_size:\n",
    "        break\n",
    "if i % 1000 != 0:\n",
    "    syllables = sorted(set(syllables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e2cb912-d603-4461-a70a-435b32f8714e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique syllables 2057\n",
      "Total syllables count from dataset 62762249\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique syllables {len(syllables)}\")\n",
    "print(f\"Total syllables count from dataset {syllables_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64d38ee4-e903-4bbd-8c0d-8cae3e5eb654",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          part    count\n",
      "ဝ            ဝ   441056\n",
      "ီ            ီ   410612\n",
      "က            က  2617855\n",
      "ပ            ပ  1788761\n",
      "ီး          ီး   167278\n",
      "...        ...      ...\n",
      "ျုရ်      ျုရ်        2\n",
      "ျာာ်      ျာာ်        1\n",
      "ှိုဏ်း  ှိုဏ်း        1\n",
      "ေါဒ်      ေါဒ်        2\n",
      "ှာား      ှာား        1\n",
      "\n",
      "[2057 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame.from_dict(syllables_hash)\n",
    "df = df.transpose()\n",
    "print(df)\n",
    "df.to_csv(\"syllables_part.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "9d7d6004-3e30-4686-a947-7e0f8ea9cf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# article to sentence\n",
    "def article_to_sentence(article):\n",
    "    sentences = re.split(r'\\u104b[\\n ]?|\\n',article)\n",
    "    return sentences\n",
    "    \n",
    "# sentence to phrase\n",
    "def sentence_to_phrase(sentence):\n",
    "    phrases = re.split(r'\\s+|\\u104a', sentence)\n",
    "    return phrases\n",
    "\n",
    "# phrase to syllable\n",
    "def phrase_to_syllable(phrase):\n",
    "    sylls = extract_syllables(phrase)\n",
    "    return sylls\n",
    "\n",
    "# get trigram from sentence\n",
    "def get_trigram_from_sentence(sentence):\n",
    "    phrases = sentence_to_phrase(sentence)\n",
    "    trigrams = []\n",
    "    if not phrases:\n",
    "        return []\n",
    "    for pidx in range(0,len(phrases)):\n",
    "        eos = 2 if pidx==len(phrases)-1 else 0\n",
    "        sylls = phrase_to_syllable(phrases[pidx])\n",
    "        if len(sylls) < 3 :\n",
    "            continue\n",
    "        for i in range(2, len(sylls)):\n",
    "            eop = 1 if i==len(sylls)-1 else 0\n",
    "            state = eop | eos\n",
    "            #trigrams.append(''.join([sylls[i-2], sylls[i-1], sylls[i],str(state)]))\n",
    "            trigrams.append(''.join([sylls[i-2], sylls[i-1], sylls[i]]))\n",
    "    return trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "90a42bce-fa78-4798-b331-4adb7f5b167c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_str = \"မြန်မာနိုင်ငံတွင် ပင်းယခေတ်နှင့် ကုန်းဘောင်ခေတ်အတွင်း အဘိဓာန် အစောင်စောင် ပေါ်ထွက်ခဲ့ဘူးသည်။ သို့သော် ယင်းအဘိဓာန်တို့သည် မြန်မာအနက်နှင့် ယှဉ်တွဲဖော်ပြထားသော ပါဠိ ဝေါဟာရ စာရင်းမျိုးသာ ဖြစ်ကြသည်။ ဖျာပုံမြို့နေ အရှင်ဩဘာသသည် ဒုတိယကမ္ဘာစစ်ကြီး မဖြစ်ပွားမီအချိန်က မြန်မာဝေါဟာရကို မြန်မာလို အနက်ဖွင့်သော အဘိဓာန်တစ်စောင်ကို ပြုစုသည်။ သို့သော် ယင်း အဘိဓာန်စာမူတို့သည် စစ်အတွင်းက ပျောက်ဆုံးခဲ့သည်ဟု သိရ၏။ စစ်ပြီးခေတ်တွင် ပြန်လည်ပြုစုရာ၊ ၁၉၄၇ ခုနှစ်တွင် အဘိဓာန်နှစ်တွဲ ထုတ်ဝေနိုင်ခဲ့သည်။\\nရန်ကုန်တက္ကသိုလ်တွင် ပြုစုလျက်ရှိသော တက္ကသိုလ်မြန်မာအဘိဓာန်ကိုမူကား ဒုတိယ ကမ္ဘာစစ်ကြီး အတွင်း ၁၉၄၄ ခုနှစ်ကပင် စတင်ခဲ့သည်။ ထိုနှစ်တွင် မြန်မာနိုင်ငံတော် ပညာ့တံခွန်အသင်းကို နိုင်ငံတော် အစိုးရ၏ အကူအညီဖြင့် တည်ထောင်သည်။ အသင်း၏ ရည်ရွယ်ချက်မှာ မြန်မာ့ ယဉ်ကျေးမှုအရပ်ရပ်ကို လေ့လာသုံးသပ် ပြန်လည် ဖော်ထုတ် ရန် ဖြစ်၏။\"\n",
    "sentences = article_to_sentence(test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "34182abe-4eb8-49c4-aea9-283ed6c280cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_output = []\n",
    "for sentence in sentences:\n",
    "    trigram_output += get_trigram_from_sentence(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "68d7abb7-6492-46e9-95f3-cf1701347001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class ZGDetector:\n",
    "    zawgyi_regex = r\"\\u1031\\u103b\"  # e+medial ra\n",
    "    zawgyi_regex += r\"|^[\\u1031\\u103b]\"  # beginning e or medial ra\n",
    "    zawgyi_regex += r\"|[\\u1022-\\u1030\\u1032-\\u1039\\u103b-\\u103d\\u1040-\\u104f]\\u103b\"  # independent vowel, dependent vowel, tone , medial ra wa ha (no ya because of 103a+103b is valid in unicode) , digit , symbol + medial ra\n",
    "    zawgyi_regex += r\"|\\u1039$\"  # end with asat\n",
    "    zawgyi_regex += r\"|\\u103d\\u103c\"  # medial ha + medial wa\n",
    "    zawgyi_regex += r\"|\\u103b\\u103c\"  # medial ra + medial wa\n",
    "    zawgyi_regex += r\"|[\\u1000-\\u1021]\\u1039[\\u101a\\u101b\\u101d\\u101f\\u1022-\\u102a\\u1031\\u1037-\\u1039\\u103b\\u1040-\\u104f]\"  # consonant + asat + ya ra wa ha independent vowel e dot below visarga asat medial ra digit symbol\n",
    "    zawgyi_regex += r\"|\\u102e[\\u102d\\u103e\\u1032]\"  # II+I II ae\n",
    "    zawgyi_regex += r\"|\\u1032[\\u102d\\u102e]\"  # ae + I II\n",
    "    # zawgyi_regex += r\"|[\\u102d\\u102e][\\u102d\\u102e]\"  # I II , II I, I I, II II  # [FIXED!! It is not so valuable zawgyi pattern ]\n",
    "    # zawgyi_regex += r\"|[\\u102f\\u1030][\\u102f\\u1030]\"  # U UU + U UU  # [FIXED!! It is not so valuable zawgyi pattern ]\n",
    "    # zawgyi_regex += r\"|[\\u102b\\u102c][\\u102b\\u102c]\"  # tall aa short aa  # [FIXED!! It is not so valuable zawgyi pattern ]\n",
    "    zawgyi_regex += r\"|[\\u1090-\\u1099][\\u102b-\\u1030\\u1032\\u1037\\u103c-\\u103e]\"  # shan digit + vowel\n",
    "    zawgyi_regex += r\"|[\\u1000-\\u102a]\\u103a[\\u102c-\\u102e\\u1032-\\u1036]\"  # consonant + medial ya + dependent vowel tone asat\n",
    "    zawgyi_regex += r\"|[\\u1023-\\u1030\\u1032-\\u1039\\u1040-\\u104f]\\u1031\"  # independent vowel dependent vowel tone digit + e [ FIXED !!! - not include medial ]\n",
    "    zawgyi_regex += r\"|[\\u107e-\\u1084][\\u1001\\u1003\\u1005-\\u100f\\u1012-\\u1014\\u1016-\\u1018\\u101f]\"  # other shapes of medial ra + consonant not in Shan consonant\n",
    "    zawgyi_regex += r\"|\\u1025\\u1039\"  # u + asat\n",
    "    zawgyi_regex += r\"|[\\u1081\\u1083]\\u108f\"  # eain-dray\n",
    "    zawgyi_regex += r\"|\\u108f[\\u1060-\\u108d]\"  # short na + stack characters\n",
    "    zawgyi_regex += r\"|[\\u102d-\\u1030\\u1032\\u1036\\u1037]\\u1039\"  # I II ae dow bolow above + asat typing error\n",
    "    zawgyi_regex += r\"|\\u102c\\u1039\"  # aa + asat awww\n",
    "    zawgyi_regex += r\"|\\u101b\\u103c\"  # ya + medial wa\n",
    "    zawgyi_regex += r\"|[^\\u1040-\\u1049]\\u1040\\u102d\"  # non digit + zero + \\u102d (i vowel) [FIXED!!! rules tested zero + i vowel in numeric usage]\n",
    "    zawgyi_regex += r\"|\\u1031?\\u1040[\\u102b\\u105a\\u102e-\\u1030\\u1032\\u1036-\\u1038]\"  # e + zero + vowel\n",
    "    zawgyi_regex += r\"|\\u1031?\\u1047[\\u102c-\\u1030\\u1032\\u1036-\\u1038]\"  # e + seven + vowel\n",
    "    # zawgyi_regex += r\"|[\\u1000-\\u1021]\\u103A[\\u1000-\\u1021]\\u1039\"  # cons + asat + cons + virama  # [ FIXED!!! REMOVED!!! conflict with Mon's Medial ]\n",
    "    zawgyi_regex += r\"|[\\u102f\\u1030\\u1032]\\u1094\"  # U | UU | AI + (zawgyi) dot below\n",
    "    zawgyi_regex += r\"|\\u1039[\\u107E-\\u1084]\"  # virama + (zawgyi) medial ra\n",
    "\n",
    "\n",
    "    def count_zawgyi_pattern(self, input_string):\n",
    "        found = 0\n",
    "        zawgyi_pattern = re.compile(self.zawgyi_regex)\n",
    "        matcher = zawgyi_pattern.finditer(input_string)\n",
    "        for match in matcher:\n",
    "            found += 1\n",
    "        return found\n",
    "\n",
    "    def is_zawgyi(self, input_string):\n",
    "        zawgyi_pattern = re.compile(self.zawgyi_regex)\n",
    "        matcher = zawgyi_pattern.search(input_string)\n",
    "        return bool(matcher)\n",
    "\n",
    "    def count_zawgyi_pattern_and_show_pattern(self, input_string):\n",
    "        found = 0\n",
    "        zawgyi_pattern = re.compile(self.zawgyi_regex)\n",
    "        matcher = zawgyi_pattern.finditer(input_string)\n",
    "\n",
    "        for match in matcher:\n",
    "            found += 1\n",
    "            print(f\"found {found} : {match.group()}\")\n",
    "\n",
    "        print(found)\n",
    "        return found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5ac268-e04e-41ec-868a-a394632111ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb8384c-9abb-46e8-b025-fc6821e78bf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "17de1386-7fbf-4ead-84d9-11ee84975f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zg_detector_rgx = ZGDetector()\n",
    "zg_detector_rgx.is_zawgyi('မျန်')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "1ee60b46-7eaf-4b51-9dbb-98dff876cd12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed206794a7d4ac8a7f95ae546ecab59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/110833 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique trigrams 1948554. Skipped 81 of 110833 articles.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "trigrams_hash={}\n",
    "ds_size = ds.num_rows\n",
    "#ds_size = 1\n",
    "skipped = 0\n",
    "for data in tqdm(ds, total=ds_size, desc=\"Processing records\"):\n",
    "    score = zg_detector.get_zawgyi_probability(data[\"text\"])\n",
    "    if score > 0.80:\n",
    "        #print(f\"zawgyi encoded sentence skipped with scoreb {score}\")\n",
    "        skipped += 1\n",
    "        continue\n",
    "    sentences = article_to_sentence(data['text'])\n",
    "    for sentence in sentences:\n",
    "        zawgyi = zg_detector_rgx.is_zawgyi(sentence)\n",
    "        if zawgyi:\n",
    "            #print(f'skipped sentence')\n",
    "            continue\n",
    "        trigrams = get_trigram_from_sentence(sentence)\n",
    "        if len(trigrams)<1:\n",
    "            continue\n",
    "        for trigram in trigrams:\n",
    "            if trigram in trigrams_hash:\n",
    "                trigrams_hash[trigram][\"count\"] += 1\n",
    "            else:\n",
    "                trigrams_hash[trigram] = {\"count\":1}\n",
    "print(f'Total unique trigrams {len(trigrams_hash)}.', f'Skipped {skipped} of {ds_size} articles.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "a228ae60-aaf4-441a-9caa-9902a1b1a633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1948554"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trigrams_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "95aed412-a4ad-4d45-95e1-4d38c629cff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(trigrams_hash)\n",
    "df = df.transpose()\n",
    "#df.to_csv(\"trigrams.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "be64e45e-4a8d-4a7e-85e9-8ca54e12646c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e441507-30c9-43cb-bd7b-bf1f1c17687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "split_size = math.ceil(df.shape[0]/(1024*1024))\n",
    "print(split_size)\n",
    "dfs= np.array_split(df,split_size)\n",
    "print(split_size, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "f3519326-bebc-474e-8203-519bfe92f134",
   "metadata": {},
   "outputs": [],
   "source": [
    "chidx = 0\n",
    "for chank in dfs:\n",
    "    chidx += 1\n",
    "    chank.to_csv(f'trigrams_chank_{chidx}.csv', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "efe4d758-0ba6-469c-a524-5854f9b7f33e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2146015"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['count']<5].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "7616b006-53fa-4021-b087-0e8081322ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered=df.loc[df['count']>5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "306f37c4-966e-478a-9c6c-73ff2e08e965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262523"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "d3d33cd2-68a8-49b7-af10-db953915a7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.to_csv('trigrams_filtered.csv', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "73289a2f-837f-41f6-820b-6e72b17320c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cecfccaac1b492eb380ec6390dd1012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Getting Unique Syllable:   0%|          | 0/285297 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "syllables_hash = {}\n",
    "for row in tqdm(df_filtered.itertuples(), total=df_filtered.shape[0], desc=\"Getting Unique Syllable\"):\n",
    "    syllables_out = extract_syllables(row.Index)\n",
    "    for syll in syllables_out:\n",
    "        if syll in syllables_hash:\n",
    "            syllables_hash[syll][\"count\"] += 1\n",
    "        else:\n",
    "            syllables_hash[syll] = {\"part\":syll, \"count\":1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "0c534216-97ef-4f5c-9177-e85a9661327c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(syllables_hash)\n",
    "df = df.transpose()\n",
    "df.to_csv(\"syllables_filtered.csv\", encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "094f7b6e-eaf5-4363-b404-4cab58330515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc6b014f308e4d2894084761743bf995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Getting Unique Syllable:   0%|          | 0/285297 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "syllables_hash = {}\n",
    "for row in tqdm(df_filtered.itertuples(), total=df_filtered.shape[0], desc=\"Getting Unique Syllable\"):\n",
    "    syllables_out = extract_syllable_to_2parts(row.Index)\n",
    "    for syll in syllables_out:\n",
    "        if syll in syllables_hash:\n",
    "            syllables_hash[syll][\"count\"] += 1\n",
    "        else:\n",
    "            syllables_hash[syll] = {\"part\":syll, \"count\":1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "6b2c11fb-f648-43d3-a794-60b4dbc8bc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(syllables_hash)\n",
    "df = df.transpose()\n",
    "df.to_csv(\"syllables_part_filtered.csv\", encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "e6d768bd-fb80-48e6-9435-ea45ea27d2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyanmarTokenizer:\n",
    "    def __init__(self):\n",
    "        cons = \"\\u1000-\\u1021\"\n",
    "        independent_vowel = \"[\\u1023-\\u1027\\u1029\\u102a]\"\n",
    "        cons_init = f\"[{cons}\\u1025\\u1026\\u1027\\u1029\\u103f]\"\n",
    "        # consonants + independent_vowel (!caution! 103F is need extra requirement)\n",
    "        medial = \"[\\u103b-\\u103e]{,4}\"\n",
    "        vowel = \"[\\u102b-\\u1032]{1,2}\\u103a?\"\n",
    "        asat = \"\\u103a\"\n",
    "        virama = \"\\u1039\"\n",
    "        cons_asat = f\"[{cons}](?:{asat}{virama}|[{asat}{virama}])\"\n",
    "        kinzi = f\"[\\u1004\\u101b]{asat}{virama}\"\n",
    "        tone = \"[\\u1036\\u1037\\u1038]{1,2}\"\n",
    "        legaund = f\"\\u104e\\u1004{asat}\\u1038|\\u104e\\u1004{asat}\"\n",
    "        symbol1 = f\"[\\u104c\\u104d\\u104f]|{legaund}|\\u104e\"\n",
    "        symbol2 = f\"\\u103b{asat}\"\n",
    "        contra1 = \"\\u103b\\u102c\"\n",
    "        digit = \"[\\u1040-\\u1049,\\.]+\"\n",
    "        # others = \"(?![a-zA-Z0-9])[\\t-~\\x0a\\x0d\\u104a\\u104b]\"\n",
    "        others = \"[ \\u104a\\u104b]\"\n",
    "        syllable_pattern = f\"(?<={cons_init}){medial}(?:{vowel})?\" + \\\n",
    "            f\"(?:{cons_asat}(?:{contra1})?)?(?:{tone})?\"\n",
    "        self.syllable_break_pattern = (\n",
    "            rf\"{symbol1}|\"\n",
    "            rf\"{symbol2}|\"\n",
    "            rf\"{syllable_pattern}|\"\n",
    "            rf\"{independent_vowel}|\"\n",
    "            rf\"{others}|\"\n",
    "            rf\"{cons_init}\"\n",
    "        )\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        syllables = re.findall(self.syllable_break_pattern, text)\n",
    "        syllables = list(filter(None, syllables))\n",
    "        return syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "cf34c632-088e-482c-9d5c-191a77b6c98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('မြန်မာနိုင်ငံ', 100)]\n",
      "[('မြန်မာနိုင်ငံ', 100)]\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "class PatriciaTrieNode:\n",
    "    def __init__(self, index, word=None):  # Now stores an index\n",
    "        self.index = index       # Index into the syllable/sound array\n",
    "        self.word = word         # The complete word if this node is a terminal node\n",
    "        self.children = {}       # Dictionary for child nodes (index -> node)\n",
    "\n",
    "class PatriciaTrie:\n",
    "    def __init__(self, work_frequencies):\n",
    "        self.root = PatriciaTrieNode(None)  # Root has no index\n",
    "        self.syllable_index = []  # Array to store unique syllables/sounds\n",
    "        self.index_map = {} # Dictionary to store syllable to index mapping\n",
    "        self.myn_tokenizer = MyanmarTokenizer()\n",
    "        self.word_frequencies = work_frequencies\n",
    "        \n",
    "    def insert(self, word):\n",
    "        syllables = self._tokenize(word)\n",
    "        node = self.root\n",
    "\n",
    "        for syllable in syllables:\n",
    "            index = self._get_or_create_index(syllable) # Get or create index\n",
    "\n",
    "            if index in node.children:\n",
    "                node = node.children[index]\n",
    "            else:\n",
    "                new_node = PatriciaTrieNode(index, word)\n",
    "                node.children[index] = new_node\n",
    "                node = new_node\n",
    "                return # Important: Stop after creating the new node\n",
    "\n",
    "    def suggest_with_scores(self, prefix, top_n=3):\n",
    "        syllables = self._tokenize(prefix)\n",
    "        node = self.root\n",
    "\n",
    "        for syllable in syllables:\n",
    "            index = self._get_index(syllable)\n",
    "            if index is None:\n",
    "                return []\n",
    "\n",
    "            if index in node.children:\n",
    "                node = node.children[index]\n",
    "            else:\n",
    "                return []\n",
    "\n",
    "        # Use a heap to efficiently get the top-N suggestions with scores\n",
    "        heap = []\n",
    "        self._get_words_with_scores_from_node(node, heap)  # Populate the heap\n",
    "\n",
    "        # Get the top-N from the heap\n",
    "        top_suggestions = heapq.nlargest(top_n, heap, key=lambda item: item[1])\n",
    "        return top_suggestions\n",
    "\n",
    "    def _get_words_with_scores_from_node(self, node, heap, current_score=0):\n",
    "        if node.word:\n",
    "            score = self.word_frequencies.get(node.word, 0)  # Default score of 0 if not found\n",
    "            heapq.heappush(heap, (node.word, score))\n",
    "\n",
    "        for child in node.children.values():\n",
    "            self._get_words_with_scores_from_node(child, heap, current_score + 1) # Example score increment\n",
    "\n",
    "    def _tokenize(self, word):\n",
    "        # Your existing syllable breaking logic here...\n",
    "        return self.myn_tokenizer.tokenize(word)\n",
    "        \n",
    "    def _get_or_create_index(self, syllable):\n",
    "        if syllable in self.index_map:\n",
    "            return self.index_map[syllable]\n",
    "\n",
    "        self.syllable_index.append(syllable)\n",
    "        index = len(self.syllable_index) - 1\n",
    "        self.index_map[syllable] = index\n",
    "        return index\n",
    "\n",
    "    def _get_index(self, syllable):\n",
    "        return self.index_map.get(syllable) #return None if not found\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "test_freq = {\"မြန်မာနိုင်ငံ\": 100, \"မြန်မာလူမျိုး\": 50, \"မြန်မာဘာသာ\": 75, \"မြန်မာစကား\": 120}  # Replace with your actual frequencies\n",
    "\n",
    "trie = PatriciaTrie(test_freq)\n",
    "trie.insert(\"မြန်မာစာ\")\n",
    "trie.insert(\"မြန်မာလူမျိုး\")\n",
    "trie.insert(\"မြန်မာဘာသာစကား\")\n",
    "trie.insert(\"မြန်မာနိုင်ငံ\")  # Add more words for testing\n",
    "\n",
    "suggestions_with_scores = trie.suggest_with_scores(\"မြန်မာ\", top_n=3)\n",
    "print(suggestions_with_scores)  # Output: [('myanmar', 7), ('myaing', 6), ('myat', 4)] (Example scores)\n",
    "\n",
    "suggestions_with_scores = trie.suggest_with_scores(\"မြန်မာ\", top_n=2)\n",
    "print(suggestions_with_scores)  # Output: [('myanmar', 7), ('myaing', 6)] (Example scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "d35760f2-5d93-488a-9e74-33bb87e1d10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams_ds = df_filtered.to_dict().get('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "82d5366a-5bc1-4ef0-bc2f-d789d03c29d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trie = PatriciaTrie(trigrams_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "ad513bae-5172-4b94-8900-eba30d7fad6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be34ef0218764cd1a07e6e6c2fe77d19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inserting dictionary in Patricia Trie:   0%|          | 0/262523 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for trigram in tqdm(trigrams_ds, total=len(trigrams_ds), desc=\"Inserting dictionary in Patricia Trie\"):\n",
    "    trie.insert(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "05037d9e-775b-4996-add2-e993c1dd2f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('မြန်မာလူ', 783), ('မြန်မာလို', 160), ('မြန်မာလက်', 124), ('မြန်မာလ', 35)]"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trie.suggest_with_scores(\"မြန်မာလ\", top_n=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
